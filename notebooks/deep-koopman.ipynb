{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ba0287",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "We wish to learn a generic, continuous system $\\mathbf f$ with a state space $\\mathbf x \\in \\mathcal X \\subseteq \\mathbf R^N$ and an input space $\\mathbf u \\in \\mathcal U \\subseteq \\mathbf R^M$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\dot{\\mathbf x} = \\mathbf f(\\mathbf x, \\mathbf u),\n",
    "\\end{equation}\n",
    "\n",
    "from a set of trajectories $\\mathcal T = \\{[(\\mathbf x_{11}, \\mathbf u_{11}), \\dots, (\\mathbf x_{n_11}, \\mathbf u_{n_11})], \\dots, [(\\mathbf x_{1m}, \\mathbf u_{1m}), \\dots, (\\mathbf x_{n_mm}, \\mathbf u_{n_mm})]\\}$ where each trajectory in $t_i \\in \\mathcal T$ has an arbitrary length ${n_i}$. We find the system state after some time $\\tau$ by solving the initial value problem on the initial state $\\mathbf x_0$.\n",
    "\n",
    "\n",
    "### Koopman Invariant Subspace\n",
    "\n",
    "Rather than learn $\\mathbf f$ generically, we wish to use *Koopman Linearization*, a globally linear representation of the system. Koopman linearization utilizes a new space, the image of the states under an *observables* function $\\mathbf g: \\mathcal X \\rightarrow \\mathcal H$, where the dynamics are governed by a linear operator $\\mathcal K$, the so-called *Koopman Operator*. This space $\\mathcal H$ is a Hilbert space with typically infinite dimensions, making the approximation of the Koopman operator challenging. Usually, the goal is relaxed to finding a finite dimensional operator that acts on a subspace where the dynamics are *weakly linear*, recovering a *Koopman invariant subspace*. It is worth noting that techniques exist which do recover an infinite dimensional linear operator, often using things like the *kernel trick*, but we value having access to the operator explicitely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2209e25",
   "metadata": {},
   "source": [
    "## Deep Learning Approach\n",
    "\n",
    "### Baseline Formulation\n",
    "\n",
    "For any number of trajectories, we can construct the matrices $\\mathbf X_1$, $\\mathbf X_2$ such that $\\mathbf F_{\\Delta t}(\\mathbf X_1) = \\mathbf X_2$, where $\\mathbf F_t$ evolves the state $\\mathbf x_0$ $t$ time into the future. Given the given Koopman linearization, this motivates the optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname*{argmin}_{\\mathbf g, \\mathcal K} \\| \\mathbf g(\\mathbf X_2) - \\mathcal K_{\\Delta t} \\mathbf g(\\mathbf X_1) \\|_{\\text{MSE}},\n",
    "\\end{equation}\n",
    "\n",
    "noting that setting $\\mathbf g(\\mathbf x) = \\mathbf x$ changes the problem to dynamic mode decomposition (DMD) and setting $\\mathbf g(\\mathbf x) = \\mathbf f(\\mathbf x)$ changes the problem to extended dynamic mode decomposition (eDMD). \n",
    "\n",
    "\n",
    "We formulate a network $G_\\Theta (\\mathbf x) = (\\mathbf g_{\\theta}^{-1} \\circ \\mathcal K_{\\theta} \\circ \\mathbf g_{\\theta}) (\\mathbf x)$ where $\\mathbf g$, $\\mathbf g^{-1}$ are a encoder/decoder pair and $\\mathcal K$ is a linear layer with no bias (matching the Koopman operator definition). Note that we have some hyperparameters $\\Theta$ that we get from typical neural network constructions. Unlike common autoencoder architectures, the latent space will likely have higher dimensionality than the original space so that the dynamics are approximately linear. Accordingly, a loss will needed to be added to account for how well the encoder embedds linear dynamics. In total, we implement the following loss functions\n",
    "\n",
    "* *Reconstruction Loss*: $\\mathcal L_{\\text{recon}}=\\left\\| \\mathbf x_1 - \\mathbf g^{-1}(\\mathbf g (\\mathbf x_1)) \\right\\|_{\\text{MSE}}$ Standard autoencoder loss.\n",
    "* *Prediction Loss*: $\\mathcal L_{\\text{pred}}=\\left\\| \\mathbf x_2 - \\mathbf g^{-1}(\\mathcal K \\mathbf g (\\mathbf x_1)) \\right\\|_{\\text{MSE}}$ How well network can predict dynamics.\n",
    "* *Linearity Loss*: $\\mathcal L_{\\text{lin}}=\\left\\| \\mathbf g(x_2) - \\mathcal K \\mathbf g (\\mathbf x_1) \\right\\|_{\\text{MSE}}$ How well the network can predict dynamics in the observables space (Koopman invariant subspace).\n",
    "* *Largest Loss*: $\\mathcal L_{\\infty}=\\left\\| \\mathbf x_1 - \\mathbf g^{-1}(\\mathbf g (\\mathbf x_1)) \\right\\|_{\\infty}$ + $\\left\\| \\mathbf x_2 - \\mathbf g^{-1}(\\mathcal K \\mathbf g (\\mathbf x_1)) \\right\\|_{\\infty}$ Loss for maximum deviation in state or observables space.\n",
    "* *Metric Loss* $\\mathcal L_{\\text{metric}}=\\left| \\left\\| \\mathbf g(x_2) - \\mathbf g(x_1) \\right\\|_{\\text{MSE}} - \\left\\| x_2 - x_1 \\right\\|_{\\text{MSE}} \\right|$ Encourage distances to be preserved in the observables space--useful for inputs that will be directly injected into observables space.\n",
    "\n",
    "for the loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal L = \\alpha_1 \\left(\\mathcal L_{\\text{recon}} + \\mathcal L_{\\text{pred}}\\right) + \\mathcal L_{\\text{lin}} + \\alpha_2 \\mathcal L_{\\infty} + \\alpha_3 \\mathcal L_{\\text{metric}} + \\alpha_4 \\| \\mathbf W \\|_2^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf W$ are the weights of the hidden layers of the form $\\mathbf W \\mathbf x + \\mathbf b$ with rectified linear units (ReLU).\n",
    "\n",
    "### Extensions to Systems with Inputs\n",
    "\n",
    "For systems with inputs, the network $G'_\\Theta (\\mathbf x, \\mathbf u) = \\left( \\mathbf g_{\\theta}^{-1} \\circ \\begin{bmatrix} \\mathcal K_{\\theta} && \\mathbf B_{\\theta}\\end{bmatrix} \\circ \\begin{bmatrix} \\mathbf g_\\theta (.)\\\\ \\mathbf u \\end{bmatrix} \\right) (\\mathbf x)$ suffices with the new linear operator $\\mathbf B_\\theta$. In the cost function, *metric loss* is added to the inputs can be injected in a space that approximately preserves the distance of the state space, where it is are added for linear systems.\n",
    "\n",
    "\n",
    "### Dimensionality Reduction in the Latent Space\n",
    "\n",
    "As commonly seen in perturbation theory, nonlinearities generate higher harmonics, and so an auxiliary network can be employed to identify the continuous eigenvalue spectrum $G'_\\Theta (\\mathbf x) = (\\mathbf g_{\\theta}^{-1} \\circ \\mathcal K_{\\theta}(\\mathbf \\lambda (\\mathbf g_\\theta (\\mathbf x))) \\circ \\mathbf g_{\\theta}) (\\mathbf x)$ . Further, if this explicit frequency dependence is unaccounted for, then a high-dimensional network is necessary to account for the shifting frequency and eigenvalues. \n",
    "\n",
    "\n",
    "\n",
    "### State Projection\n",
    "\n",
    "We can trivially implement the decoder by restricting the observables to include the states $G'_\\Theta (\\mathbf x) = \\left(\\begin{bmatrix} \\mathbf 1_{N} && \\mathbf 0\\end{bmatrix} \\circ \\mathcal K_{\\theta} \\circ \\begin{bmatrix} \\mathbf x \\\\ \\mathbf g_\\theta (.)\\end{bmatrix} \\right) (\\mathbf x)$. This is useful for set propagation-based reachability, where the ability to project sets into the original state space is helped by this relation. Propagating a set of some known representation has been of interest, and work like ReLUplex shows that an overapproximation of an image of some initial set under a neural network using ReLU nonlinearities can be done efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902f0e8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Lusch, B., Kutz, J. N., & Brunton, S. L. (2018). Deep learning for universal linear embeddings of nonlinear dynamics. Nature communications, 9(1), 1-10.\n",
    "\n",
    "Li, Y., He, H., Wu, J., Katabi, D., & Torralba, A. (2019). Learning compositional koopman operators for model-based control. arXiv preprint arXiv:1910.08264.\n",
    "\n",
    "Katz, G., Barrett, C., Dill, D. L., Julian, K., & Kochenderfer, M. J. (2017, July). Reluplex: An efficient SMT solver for verifying deep neural networks. In International conference on computer aided verification (pp. 97-117). Springer, Cham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the notebook imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# this is the convenience function\n",
    "from autokoopman import auto_koopman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a complete example, let's create an example dataset using an included benchmark system\n",
    "import autokoopman.benchmark.fhn as fhn\n",
    "dt = 0.1\n",
    "tspan = 10.0\n",
    "fhn = fhn.FitzHughNagumo()\n",
    "training_data = fhn.solve_ivps(\n",
    "    initial_states=np.random.uniform(low=-2.5, high=2.5, size=(50, 2)),\n",
    "    tspan=[0.0, dt*5],\n",
    "    sampling_period=dt\n",
    ")\n",
    "\n",
    "test_ivs = np.random.uniform(low=-2.0, high=2.0, size=(20, 2))\n",
    "test_data = fhn.solve_ivps(\n",
    "    initial_states=test_ivs,\n",
    "    tspan=[0.0, tspan],\n",
    "    sampling_period=dt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokoopman.estimator.deepkoopman as dk\n",
    "\n",
    "koop = dk.DeepKoopman(\n",
    "    state_dim=len(training_data.state_names), \n",
    "    input_dim=0, \n",
    "    hidden_dim=12, \n",
    "    max_iter=2000,\n",
    "    lr=1e-3, \n",
    "    hidden_enc_dim=64,\n",
    "    num_hidden_layers=2,  \n",
    "    metric_loss_weight=1e-2, \n",
    "    pred_loss_weight=1e-2, \n",
    "    validation_data=test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do training\n",
    "koop.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1850f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(koop.loss_hist['validation_lin_loss'], label=\"linearity loss\")\n",
    "plt.plot(koop.loss_hist['validation_pred_loss'], label=\"prediction loss\")\n",
    "plt.plot(koop.loss_hist['validation_total_loss'], label=\"total loss\")\n",
    "#plt.plot(koop.loss_hist['total_loss'], label=\"total loss\")\n",
    "#plt.plot(koop.loss_hist['validation_metric_loss'], label=\"metric loss\")\n",
    "#plt.plot(koop.loss_hist['validation_recon_loss'], label=\"reconstruction loss\")\n",
    "\n",
    "#plt.plot(koop.loss_hist['lin_loss'], label=\"linearity loss\")\n",
    "#plt.plot(koop.loss_hist['pred_loss'], label=\"prediction loss\")\n",
    "#plt.plot(koop.loss_hist['total_loss'], label=\"total loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Deep Learning Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29333a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve for test trajectories\n",
    "fit_data = koop.model.solve_ivps(test_ivs, tspan=[0.0, tspan], sampling_period=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for t in training_data:\n",
    "    plt.plot(*t.states.T, 'gray', alpha=0.3)\n",
    "for t, tt in zip(fit_data, test_data):\n",
    "    plt.plot(*t.states.T, 'r')\n",
    "    plt.plot(*tt.states.T, 'k', alpha=0.5)\n",
    "    \n",
    "plt.title(\"FHN Deep Koopman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a75955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fec21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
